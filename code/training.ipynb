{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67173e76",
   "metadata": {},
   "source": [
    "# MMI prediction\n",
    "\n",
    "## 5. Training\n",
    "- Shortlist 10 models\n",
    "- Hyperparameter tuning top 3 + LR\n",
    "- Fit tuned models on entire training set and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ccac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline as make_pipeline_sk\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9609fd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x7f8b3ef5d460>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.config_context(verbosity = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236d6f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 304 training samples\n"
     ]
    }
   ],
   "source": [
    "# load and shuffle data\n",
    "\n",
    "X_train = pd.read_pickle('X_train_trans.pkl')\n",
    "y_train = np.ravel(pd.read_pickle('y_train.pkl'))\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "print('There are {} training samples'.format(X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d4d67",
   "metadata": {},
   "source": [
    "## Shortlist several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "029b8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries of hyperparameters for sklearn and GBT models\n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "log_reg_params = [{'penalty': 'none', 'max_iter': max_iter}]\n",
    "dec_tree_params = [{'criterion': 'gini'}, {'criterion': 'entropy'}]\n",
    "rand_for_params = [{'criterion': 'gini'}, {'criterion': 'entropy'}]\n",
    "kneighbors_params = [{'n_neighbors': 3}, {'n_neighbors': 5}]\n",
    "naive_bayes_params = [{}]\n",
    "svc_params = [{'C': 0.01}, {'C': 0.1}, {'C': 1}, {'C': 10}]\n",
    "xgb_params = [{'use_label_encoder': False}]\n",
    "cb_params = [{'verbose': False}]\n",
    "lgbm_params = [{}]\n",
    "mlp_params = [{'hidden_layer_sizes': (10,), 'max_iter': max_iter}, {'hidden_layer_sizes': (10, 10,), 'max_iter': max_iter}, \n",
    "              {'hidden_layer_sizes': (10, 10, 10,), 'max_iter': max_iter}]\n",
    "\n",
    "models = [\n",
    "    ['log regression', LogisticRegression, log_reg_params],\n",
    "    ['decision tree', DecisionTreeClassifier, dec_tree_params],\n",
    "    ['random forest', RandomForestClassifier, rand_for_params],\n",
    "    ['k neighbors', KNeighborsClassifier, kneighbors_params],\n",
    "    ['naive bayes', GaussianNB, naive_bayes_params],\n",
    "    ['support vector machines', SVC, svc_params],\n",
    "    ['XG boost', xgb.XGBClassifier, xgb_params],\n",
    "    ['Cat boost', CatBoostClassifier, cb_params],\n",
    "    ['Light GBM', LGBMClassifier, lgbm_params],\n",
    "    ['MLP', MLPClassifier, mlp_params]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d64532a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to evaluate sklearn and GBT models\n",
    "\n",
    "def eval_models(models, score, X_train, y_train, fname):\n",
    "    results = []\n",
    "    result_file = open(fname, 'a')\n",
    "    \n",
    "    for model_name, Model, params_list in models:\n",
    "        for params in params_list:\n",
    "            model = make_pipeline(\n",
    "                SMOTE(),\n",
    "                Model(**params)\n",
    "            )\n",
    "            cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 10)\n",
    "            scores = list(cross_val_score(model, X_train, y_train, scoring = score, cv = cv))\n",
    "            results.append((model_name, model, params, np.mean(scores), np.std(scores), scores))\n",
    "    \n",
    "    results.sort(key = lambda x:x[-3], reverse = True)\n",
    "    \n",
    "    # write score summary to txt file\n",
    "    result_file.write('\\nmean {} scores:\\n\\n'.format(score))\n",
    "    for modelname, model, params, mean, std, scores in results:\n",
    "        result_file.write(str(modelname) + '\\t' + str(params) + '\\t' + str(mean) + '\\n')\n",
    "    result_file.close()\n",
    "    \n",
    "    # write scores to dataframe\n",
    "    df = pd.DataFrame()\n",
    "    for modelname, model, params, mean, std, scores in results:\n",
    "        column_name = str(modelname) + str(params)\n",
    "        df[column_name] = scores\n",
    "    df.to_pickle('experiments/init_training_results_{}.pkl'.format(score))\n",
    "    \n",
    "    # write permutation feature importances to dataframe\n",
    "    if score == 'roc_auc':\n",
    "        for modelname, model, params, mean, std, scores in results:\n",
    "            model.fit(X_train, y_train)\n",
    "            feat_imp = permutation_importance(model, X_train, y_train, n_repeats = 10)\n",
    "            feat_imp_df = pd.DataFrame(feat_imp.importances, index = X_train.columns.tolist())\n",
    "            feat_imp_df.to_pickle('experiments/init_training_feat_imp_{}.pkl'.format(modelname).replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4648658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:25:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:25:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:25:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:34:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:40:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:40:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:40:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:46:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:46:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:46:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:52:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:52:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:52:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:53:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:53:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# evaluate sklearn/GBT models\n",
    "\n",
    "scores = ['roc_auc', 'precision', 'recall', 'f1', 'accuracy']\n",
    "fname = 'experiments/init_training_summary' + '_' + str(datetime.now().year) + '_' + str(datetime.now().month) \\\n",
    "    + '_' + str(datetime.now().day) + '_' + str(datetime.now().hour) + '_' + \\\n",
    "    str(datetime.now().minute) + '.txt'\n",
    "        \n",
    "for score in scores:\n",
    "    eval_models(models = models, score = score, X_train = X_train, y_train = y_train, fname = fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa691a49",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "287f7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested CV to find best parameters for sklearn models\n",
    "\n",
    "def eval_params(fname, tuning_model, param_grid):\n",
    "    \n",
    "    results = []\n",
    "    result_file = open(fname, 'a')\n",
    "\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "    fold_no = 1\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "\n",
    "        X_train_split, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_split, y_test = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        # find best model params\n",
    "        print('finding best model parameters for fold number {}'.format(fold_no))\n",
    "        model = make_pipeline(\n",
    "            SMOTE(),\n",
    "            tuning_model\n",
    "        )\n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs = -1, cv = 5, error_score = 'raise', \n",
    "                           scoring = 'roc_auc', verbose = 3)\n",
    "        grid_result = grid.fit(X_train_split, y_train_split)\n",
    "        \n",
    "        # evaluate best model params on outer fold\n",
    "        print('evaluating model for fold number {}'.format(fold_no))\n",
    "        best_params = grid_result.best_params_\n",
    "        print(best_params)\n",
    "        best_model = make_pipeline_sk(tuning_model)\n",
    "        best_model.set_params(**best_params)\n",
    "        best_model.fit(X_train_split, y_train_split)\n",
    "        score = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "        results.append((best_params, score))\n",
    "        print('parameters: {}'.format(str(best_params)))\n",
    "        print('AUC score: {}'.format(score))\n",
    "        fold_no += 1\n",
    "\n",
    "    results.sort(key = lambda x:x[-1], reverse = True)\n",
    "    result_file.write('\\nAUC scores:\\n\\n')\n",
    "    \n",
    "    # write score summary to text file\n",
    "    for best_params, score in results:\n",
    "        result_file.write(str(best_params) + '\\t' + str(score) + '\\n')\n",
    "    result_file.close()\n",
    "    \n",
    "    # write scores to dataframe:\n",
    "    df = pd.DataFrame(results, columns = ['params', 'score'])\n",
    "    df.to_pickle('experiments/hyperparameter_tuning_results_{}.pkl'.format(tuning_model).replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define param grid for grid search - random forest\n",
    "\n",
    "n_estimators = [int(x) for x in range(20, 200, 20)] \n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10] \n",
    "min_samples_leaf = [1, 2, 4]\n",
    "rf_grid = {'randomforestclassifier__n_estimators': n_estimators, \n",
    "          'randomforestclassifier__max_features': max_features, \n",
    "          'randomforestclassifier__max_depth': max_depth, \n",
    "          'randomforestclassifier__min_samples_split': min_samples_split, \n",
    "          'randomforestclassifier__min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bbc5289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding best model parameters for fold number 1\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "evaluating model for fold number 1\n",
      "{'randomforestclassifier__max_depth': 60, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__n_estimators': 60}\n",
      "parameters: {'randomforestclassifier__max_depth': 60, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__n_estimators': 60}\n",
      "AUC score: 0.7488207547169812\n",
      "finding best model parameters for fold number 2\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "evaluating model for fold number 2\n",
      "{'randomforestclassifier__max_depth': None, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__min_samples_split': 5, 'randomforestclassifier__n_estimators': 140}\n",
      "parameters: {'randomforestclassifier__max_depth': None, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__min_samples_split': 5, 'randomforestclassifier__n_estimators': 140}\n",
      "AUC score: 0.8372641509433962\n",
      "finding best model parameters for fold number 3\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "evaluating model for fold number 3\n",
      "{'randomforestclassifier__max_depth': 90, 'randomforestclassifier__max_features': 'sqrt', 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 5, 'randomforestclassifier__n_estimators': 100}\n",
      "parameters: {'randomforestclassifier__max_depth': 90, 'randomforestclassifier__max_features': 'sqrt', 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 5, 'randomforestclassifier__n_estimators': 100}\n",
      "AUC score: 0.7358490566037736\n",
      "finding best model parameters for fold number 4\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "evaluating model for fold number 4\n",
      "{'randomforestclassifier__max_depth': 80, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 10, 'randomforestclassifier__n_estimators': 40}\n",
      "parameters: {'randomforestclassifier__max_depth': 80, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 10, 'randomforestclassifier__n_estimators': 40}\n",
      "AUC score: 0.7287735849056605\n",
      "finding best model parameters for fold number 5\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "evaluating model for fold number 5\n",
      "{'randomforestclassifier__max_depth': 60, 'randomforestclassifier__max_features': 'sqrt', 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__n_estimators': 40}\n",
      "parameters: {'randomforestclassifier__max_depth': 60, 'randomforestclassifier__max_features': 'sqrt', 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__n_estimators': 40}\n",
      "AUC score: 0.8113207547169811\n"
     ]
    }
   ],
   "source": [
    "# tune random forest\n",
    "\n",
    "fname = 'experiments/rf_tuning' + '_' + str(datetime.now().year) + '_' + str(datetime.now().month) \\\n",
    "    + '_' + str(datetime.now().day) + '_' + str(datetime.now().hour) + '_' + \\\n",
    "    str(datetime.now().minute) + '.txt'\n",
    "\n",
    "eval_params(fname = fname, tuning_model = RandomForestClassifier(), param_grid = rf_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c367f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define param grid for grid search - SVM\n",
    "\n",
    "C = np.logspace(-2, 3, num = 6)\n",
    "gamma = np.logspace(-4, 0, num = 5)\n",
    "kernel = ['rbf']\n",
    "probability = [True]\n",
    "\n",
    "svm_grid = {'svc__C': C, \n",
    "           'svc__gamma': gamma, \n",
    "           'svc__kernel': kernel,\n",
    "           'svc__probability': probability}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0f1f5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding best model parameters for fold number 1\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "evaluating model for fold number 1\n",
      "{'svc__C': 1000.0, 'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "parameters: {'svc__C': 1000.0, 'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "AUC score: 0.8089622641509434\n",
      "finding best model parameters for fold number 2\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "evaluating model for fold number 2\n",
      "{'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "parameters: {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "AUC score: 0.8160377358490566\n",
      "finding best model parameters for fold number 3\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "evaluating model for fold number 3\n",
      "{'svc__C': 0.01, 'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "parameters: {'svc__C': 0.01, 'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "AUC score: 0.6627358490566038\n",
      "finding best model parameters for fold number 4\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "evaluating model for fold number 4\n",
      "{'svc__C': 1.0, 'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "parameters: {'svc__C': 1.0, 'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "AUC score: 0.8419811320754718\n",
      "finding best model parameters for fold number 5\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "evaluating model for fold number 5\n",
      "{'svc__C': 1000.0, 'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "parameters: {'svc__C': 1000.0, 'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__probability': True}\n",
      "AUC score: 0.8005390835579514\n"
     ]
    }
   ],
   "source": [
    "# tune SVM\n",
    "\n",
    "fname = 'experiments/svm_tuning' + '_' + str(datetime.now().year) + '_' + str(datetime.now().month) \\\n",
    "    + '_' + str(datetime.now().day) + '_' + str(datetime.now().hour) + '_' + \\\n",
    "    str(datetime.now().minute) + '.txt'\n",
    "\n",
    "eval_params(fname = fname, tuning_model = SVC(), param_grid = svm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1687ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define param grid for grid search - log reg\n",
    "\n",
    "penalty = ['l2', 'none']\n",
    "C = np.logspace(-2, 2, num = 5)\n",
    "max_iter = [1000]\n",
    "\n",
    "logreg_grid = {'logisticregression__penalty': penalty, \n",
    "              'logisticregression__C': C, \n",
    "              'logisticregression__max_iter': max_iter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6b152d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding best model parameters for fold number 1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "evaluating model for fold number 1\n",
      "{'logisticregression__C': 0.1, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'l2'}\n",
      "parameters: {'logisticregression__C': 0.1, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'l2'}\n",
      "AUC score: 0.8726415094339622\n",
      "finding best model parameters for fold number 2\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model for fold number 2\n",
      "{'logisticregression__C': 100.0, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'none'}\n",
      "parameters: {'logisticregression__C': 100.0, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'none'}\n",
      "AUC score: 0.6320754716981132\n",
      "finding best model parameters for fold number 3\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "evaluating model for fold number 3\n",
      "{'logisticregression__C': 0.1, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'l2'}\n",
      "parameters: {'logisticregression__C': 0.1, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'l2'}\n",
      "AUC score: 0.8018867924528302\n",
      "finding best model parameters for fold number 4\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "evaluating model for fold number 4\n",
      "{'logisticregression__C': 0.1, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'l2'}\n",
      "parameters: {'logisticregression__C': 0.1, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'l2'}\n",
      "AUC score: 0.7122641509433962\n",
      "finding best model parameters for fold number 5\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "evaluating model for fold number 5\n",
      "{'logisticregression__C': 10.0, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'l2'}\n",
      "parameters: {'logisticregression__C': 10.0, 'logisticregression__max_iter': 1000, 'logisticregression__penalty': 'l2'}\n",
      "AUC score: 0.7439353099730458\n"
     ]
    }
   ],
   "source": [
    "# tune logistic regression\n",
    "\n",
    "fname = 'experiments/logreg_tuning' + '_' + str(datetime.now().year) + '_' + str(datetime.now().month) \\\n",
    "    + '_' + str(datetime.now().day) + '_' + str(datetime.now().hour) + '_' + \\\n",
    "    str(datetime.now().minute) + '.txt'\n",
    "\n",
    "eval_params(fname = fname, tuning_model = LogisticRegression(), param_grid = logreg_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdb48bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define param grid for grid search - catboost\n",
    "\n",
    "# iterations = [5, 10, 15, 20, 25, 50, 100]\n",
    "# learning_rate = [0.01, 0.05, 0.1]\n",
    "# depth = np.arange(3, 15, 2)\n",
    "# cb_grid = {'catboostclassifier__iterations': iterations,\n",
    "#            'catboostclassifier__learning_rate': learning_rate,\n",
    "#            'catboostclassifier__depth': depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c03d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nested CV to find best parameters for catboost\n",
    "\n",
    "# def eval_cb_params(fname, param_grid):\n",
    "    \n",
    "#     results = []\n",
    "#     result_file = open(fname, 'a')\n",
    "\n",
    "#     skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "#     fold_no = 1\n",
    "    \n",
    "#     for train_index, test_index in skf.split(X_train, y_train):\n",
    "\n",
    "#         X_train_split, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "#         y_train_split, y_test = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "#         # find best model params\n",
    "#         print('finding best model parameters for fold number {}'.format(fold_no))\n",
    "#         tuning_model = CatBoostClassifier()\n",
    "#         model = make_pipeline(\n",
    "#             SMOTE(),\n",
    "#             tuning_model\n",
    "#         )\n",
    "#         grid = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs = -1, cv = 5, error_score = 'raise', \n",
    "#                            scoring = 'roc_auc', verbose = 3)\n",
    "#         grid_result = grid.fit(X_train_split, y_train_split)\n",
    "        \n",
    "#         # evaluate best model params on outer fold\n",
    "#         print('evaluating model for fold number {}'.format(fold_no))\n",
    "#         best_params = grid_result.best_params_\n",
    "#         print(best_params)\n",
    "#         best_model = make_pipeline_sk(tuning_model)\n",
    "#         best_model.set_params(**best_params)\n",
    "#         best_model.fit(X_train_split, y_train_split)\n",
    "#         score = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "#         results.append((best_params, score))\n",
    "#         print('parameters: {}'.format(str(best_params)))\n",
    "#         print('AUC score: {}'.format(score))\n",
    "#         fold_no += 1\n",
    "\n",
    "#     results.sort(key = lambda x:x[-1], reverse = True)\n",
    "#     result_file.write('\\nAUC scores:\\n\\n')\n",
    "#     for best_params, score in results:\n",
    "#         result_file.write(str(best_params) + '\\t' + str(score) + '\\n')\n",
    "#     result_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3864de4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding best model parameters for fold number 1\n",
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n",
      "0:\tlearn: 0.6013844\ttotal: 6.57ms\tremaining: 59.2ms\n",
      "1:\tlearn: 0.5396119\ttotal: 12.9ms\tremaining: 51.6ms\n",
      "2:\tlearn: 0.4912861\ttotal: 16ms\tremaining: 37.4ms\n",
      "3:\tlearn: 0.4378666\ttotal: 18.6ms\tremaining: 27.9ms\n",
      "4:\tlearn: 0.4092037\ttotal: 21.2ms\tremaining: 21.2ms\n",
      "5:\tlearn: 0.3703096\ttotal: 23.7ms\tremaining: 15.8ms\n",
      "6:\tlearn: 0.3430178\ttotal: 26.2ms\tremaining: 11.2ms\n",
      "7:\tlearn: 0.3142624\ttotal: 28.8ms\tremaining: 7.21ms\n",
      "8:\tlearn: 0.2982096\ttotal: 31.3ms\tremaining: 3.48ms\n",
      "9:\tlearn: 0.2789672\ttotal: 34.1ms\tremaining: 0us\n",
      "evaluating model for fold number 1\n",
      "{'catboostclassifier__depth': 5, 'catboostclassifier__iterations': 10, 'catboostclassifier__learning_rate': 0.1}\n",
      "0:\tlearn: 0.6138371\ttotal: 3.95ms\tremaining: 35.6ms\n",
      "1:\tlearn: 0.5463525\ttotal: 10ms\tremaining: 40.1ms\n",
      "2:\tlearn: 0.4992641\ttotal: 12.2ms\tremaining: 28.6ms\n",
      "3:\tlearn: 0.4616242\ttotal: 14.2ms\tremaining: 21.4ms\n",
      "4:\tlearn: 0.4096487\ttotal: 16.2ms\tremaining: 16.2ms\n",
      "5:\tlearn: 0.3784192\ttotal: 18.1ms\tremaining: 12ms\n",
      "6:\tlearn: 0.3535902\ttotal: 19.9ms\tremaining: 8.52ms\n",
      "7:\tlearn: 0.3325954\ttotal: 21.7ms\tremaining: 5.43ms\n",
      "8:\tlearn: 0.3142994\ttotal: 23.9ms\tremaining: 2.65ms\n",
      "9:\tlearn: 0.2951947\ttotal: 25.7ms\tremaining: 0us\n",
      "parameters: {'catboostclassifier__depth': 5, 'catboostclassifier__iterations': 10, 'catboostclassifier__learning_rate': 0.1}\n",
      "AUC score: 0.8267477203647415\n",
      "finding best model parameters for fold number 2\n",
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n",
      "0:\tlearn: 0.6849131\ttotal: 7.84ms\tremaining: 188ms\n",
      "1:\tlearn: 0.6762174\ttotal: 13.8ms\tremaining: 159ms\n",
      "2:\tlearn: 0.6693056\ttotal: 16.4ms\tremaining: 121ms\n",
      "3:\tlearn: 0.6618368\ttotal: 19.1ms\tremaining: 100ms\n",
      "4:\tlearn: 0.6535442\ttotal: 21.7ms\tremaining: 87ms\n",
      "5:\tlearn: 0.6445723\ttotal: 24.4ms\tremaining: 77.3ms\n",
      "6:\tlearn: 0.6360744\ttotal: 27.2ms\tremaining: 70ms\n",
      "7:\tlearn: 0.6270206\ttotal: 29.9ms\tremaining: 63.6ms\n",
      "8:\tlearn: 0.6210297\ttotal: 32.5ms\tremaining: 57.8ms\n",
      "9:\tlearn: 0.6124994\ttotal: 35ms\tremaining: 52.5ms\n",
      "10:\tlearn: 0.6064120\ttotal: 37.5ms\tremaining: 47.8ms\n",
      "11:\tlearn: 0.5998813\ttotal: 40.1ms\tremaining: 43.4ms\n",
      "12:\tlearn: 0.5904601\ttotal: 42.6ms\tremaining: 39.3ms\n",
      "13:\tlearn: 0.5823638\ttotal: 45ms\tremaining: 35.4ms\n",
      "14:\tlearn: 0.5753717\ttotal: 47.5ms\tremaining: 31.7ms\n",
      "15:\tlearn: 0.5706412\ttotal: 50ms\tremaining: 28.1ms\n",
      "16:\tlearn: 0.5640140\ttotal: 52.4ms\tremaining: 24.6ms\n",
      "17:\tlearn: 0.5576351\ttotal: 54.9ms\tremaining: 21.4ms\n",
      "18:\tlearn: 0.5512083\ttotal: 57.4ms\tremaining: 18.1ms\n",
      "19:\tlearn: 0.5461066\ttotal: 59.8ms\tremaining: 14.9ms\n",
      "20:\tlearn: 0.5413253\ttotal: 62.3ms\tremaining: 11.9ms\n",
      "21:\tlearn: 0.5346464\ttotal: 64.9ms\tremaining: 8.85ms\n",
      "22:\tlearn: 0.5280908\ttotal: 67.3ms\tremaining: 5.86ms\n",
      "23:\tlearn: 0.5228308\ttotal: 69.8ms\tremaining: 2.91ms\n",
      "24:\tlearn: 0.5184366\ttotal: 72.2ms\tremaining: 0us\n",
      "evaluating model for fold number 2\n",
      "{'catboostclassifier__depth': 5, 'catboostclassifier__iterations': 25, 'catboostclassifier__learning_rate': 0.01}\n",
      "0:\tlearn: 0.6841140\ttotal: 1.97ms\tremaining: 47.3ms\n",
      "1:\tlearn: 0.6754600\ttotal: 13.4ms\tremaining: 155ms\n",
      "2:\tlearn: 0.6682646\ttotal: 15.8ms\tremaining: 116ms\n",
      "3:\tlearn: 0.6602590\ttotal: 17.8ms\tremaining: 93.7ms\n",
      "4:\tlearn: 0.6522578\ttotal: 20.6ms\tremaining: 82.2ms\n",
      "5:\tlearn: 0.6438728\ttotal: 23.1ms\tremaining: 73.2ms\n",
      "6:\tlearn: 0.6369074\ttotal: 25.7ms\tremaining: 66ms\n",
      "7:\tlearn: 0.6297341\ttotal: 27.9ms\tremaining: 59.3ms\n",
      "8:\tlearn: 0.6218286\ttotal: 29.9ms\tremaining: 53.2ms\n",
      "9:\tlearn: 0.6144668\ttotal: 31.8ms\tremaining: 47.6ms\n",
      "10:\tlearn: 0.6060242\ttotal: 33.6ms\tremaining: 42.7ms\n",
      "11:\tlearn: 0.5970551\ttotal: 35.4ms\tremaining: 38.3ms\n",
      "12:\tlearn: 0.5894969\ttotal: 37.2ms\tremaining: 34.3ms\n",
      "13:\tlearn: 0.5824689\ttotal: 39ms\tremaining: 30.7ms\n",
      "14:\tlearn: 0.5758921\ttotal: 41ms\tremaining: 27.3ms\n",
      "15:\tlearn: 0.5696401\ttotal: 42.8ms\tremaining: 24.1ms\n",
      "16:\tlearn: 0.5647937\ttotal: 44.7ms\tremaining: 21ms\n",
      "17:\tlearn: 0.5588174\ttotal: 46.5ms\tremaining: 18.1ms\n",
      "18:\tlearn: 0.5523650\ttotal: 48.3ms\tremaining: 15.2ms\n",
      "19:\tlearn: 0.5470152\ttotal: 50ms\tremaining: 12.5ms\n",
      "20:\tlearn: 0.5404793\ttotal: 51.8ms\tremaining: 9.87ms\n",
      "21:\tlearn: 0.5349772\ttotal: 53.6ms\tremaining: 7.31ms\n",
      "22:\tlearn: 0.5280270\ttotal: 55.4ms\tremaining: 4.82ms\n",
      "23:\tlearn: 0.5236697\ttotal: 57.1ms\tremaining: 2.38ms\n",
      "24:\tlearn: 0.5175131\ttotal: 58.9ms\tremaining: 0us\n",
      "parameters: {'catboostclassifier__depth': 5, 'catboostclassifier__iterations': 25, 'catboostclassifier__learning_rate': 0.01}\n",
      "AUC score: 0.7082066869300911\n",
      "finding best model parameters for fold number 3\n",
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n",
      "0:\tlearn: 0.6864955\ttotal: 4.78ms\tremaining: 473ms\n",
      "1:\tlearn: 0.6794037\ttotal: 7.77ms\tremaining: 381ms\n",
      "2:\tlearn: 0.6707468\ttotal: 9.08ms\tremaining: 294ms\n",
      "3:\tlearn: 0.6666170\ttotal: 10.4ms\tremaining: 249ms\n",
      "4:\tlearn: 0.6597132\ttotal: 11.6ms\tremaining: 221ms\n",
      "5:\tlearn: 0.6530462\ttotal: 13ms\tremaining: 204ms\n",
      "6:\tlearn: 0.6459361\ttotal: 14.3ms\tremaining: 190ms\n",
      "7:\tlearn: 0.6387150\ttotal: 15.6ms\tremaining: 180ms\n",
      "8:\tlearn: 0.6316190\ttotal: 16.8ms\tremaining: 170ms\n",
      "9:\tlearn: 0.6249861\ttotal: 18.2ms\tremaining: 164ms\n",
      "10:\tlearn: 0.6197969\ttotal: 19.5ms\tremaining: 157ms\n",
      "11:\tlearn: 0.6126803\ttotal: 20.7ms\tremaining: 152ms\n",
      "12:\tlearn: 0.6067929\ttotal: 22ms\tremaining: 147ms\n",
      "13:\tlearn: 0.6012482\ttotal: 23.2ms\tremaining: 142ms\n",
      "14:\tlearn: 0.5966795\ttotal: 24.4ms\tremaining: 138ms\n",
      "15:\tlearn: 0.5898507\ttotal: 25.6ms\tremaining: 134ms\n",
      "16:\tlearn: 0.5839090\ttotal: 26.9ms\tremaining: 131ms\n",
      "17:\tlearn: 0.5778247\ttotal: 28.2ms\tremaining: 128ms\n",
      "18:\tlearn: 0.5735286\ttotal: 29.4ms\tremaining: 126ms\n",
      "19:\tlearn: 0.5689589\ttotal: 30.7ms\tremaining: 123ms\n",
      "20:\tlearn: 0.5631992\ttotal: 32ms\tremaining: 120ms\n",
      "21:\tlearn: 0.5582877\ttotal: 33.3ms\tremaining: 118ms\n",
      "22:\tlearn: 0.5524886\ttotal: 34.8ms\tremaining: 117ms\n",
      "23:\tlearn: 0.5473187\ttotal: 36.1ms\tremaining: 114ms\n",
      "24:\tlearn: 0.5423888\ttotal: 37.3ms\tremaining: 112ms\n",
      "25:\tlearn: 0.5370971\ttotal: 38.6ms\tremaining: 110ms\n",
      "26:\tlearn: 0.5336461\ttotal: 39.8ms\tremaining: 108ms\n",
      "27:\tlearn: 0.5284919\ttotal: 41.1ms\tremaining: 106ms\n",
      "28:\tlearn: 0.5234437\ttotal: 42.3ms\tremaining: 104ms\n",
      "29:\tlearn: 0.5185813\ttotal: 43.6ms\tremaining: 102ms\n",
      "30:\tlearn: 0.5150577\ttotal: 44.8ms\tremaining: 99.8ms\n",
      "31:\tlearn: 0.5111205\ttotal: 46.1ms\tremaining: 97.9ms\n",
      "32:\tlearn: 0.5070442\ttotal: 47.3ms\tremaining: 96.1ms\n",
      "33:\tlearn: 0.5025063\ttotal: 48.8ms\tremaining: 94.7ms\n",
      "34:\tlearn: 0.4986511\ttotal: 50.1ms\tremaining: 93ms\n",
      "35:\tlearn: 0.4944470\ttotal: 51.3ms\tremaining: 91.3ms\n",
      "36:\tlearn: 0.4900313\ttotal: 52.8ms\tremaining: 89.9ms\n",
      "37:\tlearn: 0.4871465\ttotal: 54.1ms\tremaining: 88.3ms\n",
      "38:\tlearn: 0.4839103\ttotal: 55.4ms\tremaining: 86.6ms\n",
      "39:\tlearn: 0.4801139\ttotal: 56.6ms\tremaining: 84.9ms\n",
      "40:\tlearn: 0.4758179\ttotal: 57.9ms\tremaining: 83.3ms\n",
      "41:\tlearn: 0.4718614\ttotal: 59.1ms\tremaining: 81.7ms\n",
      "42:\tlearn: 0.4687730\ttotal: 60.4ms\tremaining: 80ms\n",
      "43:\tlearn: 0.4650008\ttotal: 61.7ms\tremaining: 78.5ms\n",
      "44:\tlearn: 0.4616736\ttotal: 62.9ms\tremaining: 76.9ms\n",
      "45:\tlearn: 0.4585675\ttotal: 64.2ms\tremaining: 75.3ms\n",
      "46:\tlearn: 0.4549580\ttotal: 65.4ms\tremaining: 73.7ms\n",
      "47:\tlearn: 0.4517652\ttotal: 66.7ms\tremaining: 72.2ms\n",
      "48:\tlearn: 0.4485850\ttotal: 67.9ms\tremaining: 70.7ms\n",
      "49:\tlearn: 0.4460167\ttotal: 69.2ms\tremaining: 69.2ms\n",
      "50:\tlearn: 0.4427727\ttotal: 70.4ms\tremaining: 67.7ms\n",
      "51:\tlearn: 0.4396686\ttotal: 71.7ms\tremaining: 66.1ms\n",
      "52:\tlearn: 0.4360181\ttotal: 72.9ms\tremaining: 64.6ms\n",
      "53:\tlearn: 0.4333538\ttotal: 74.1ms\tremaining: 63.1ms\n",
      "54:\tlearn: 0.4311334\ttotal: 75.4ms\tremaining: 61.7ms\n",
      "55:\tlearn: 0.4283193\ttotal: 76.5ms\tremaining: 60.1ms\n",
      "56:\tlearn: 0.4253743\ttotal: 77.7ms\tremaining: 58.6ms\n",
      "57:\tlearn: 0.4226797\ttotal: 79ms\tremaining: 57.2ms\n",
      "58:\tlearn: 0.4201215\ttotal: 80.3ms\tremaining: 55.8ms\n",
      "59:\tlearn: 0.4176108\ttotal: 81.5ms\tremaining: 54.3ms\n",
      "60:\tlearn: 0.4150624\ttotal: 82.7ms\tremaining: 52.9ms\n",
      "61:\tlearn: 0.4118287\ttotal: 84ms\tremaining: 51.5ms\n",
      "62:\tlearn: 0.4091762\ttotal: 85.2ms\tremaining: 50ms\n",
      "63:\tlearn: 0.4058183\ttotal: 86.5ms\tremaining: 48.7ms\n",
      "64:\tlearn: 0.4034393\ttotal: 87.9ms\tremaining: 47.3ms\n",
      "65:\tlearn: 0.4005346\ttotal: 89.2ms\tremaining: 45.9ms\n",
      "66:\tlearn: 0.3987153\ttotal: 90.3ms\tremaining: 44.5ms\n",
      "67:\tlearn: 0.3958676\ttotal: 91.5ms\tremaining: 43.1ms\n",
      "68:\tlearn: 0.3929123\ttotal: 92.8ms\tremaining: 41.7ms\n",
      "69:\tlearn: 0.3909696\ttotal: 93.9ms\tremaining: 40.2ms\n",
      "70:\tlearn: 0.3885236\ttotal: 95.1ms\tremaining: 38.8ms\n",
      "71:\tlearn: 0.3858652\ttotal: 96.2ms\tremaining: 37.4ms\n",
      "72:\tlearn: 0.3837609\ttotal: 97.5ms\tremaining: 36.1ms\n",
      "73:\tlearn: 0.3805629\ttotal: 98.8ms\tremaining: 34.7ms\n",
      "74:\tlearn: 0.3783602\ttotal: 100ms\tremaining: 33.3ms\n",
      "75:\tlearn: 0.3757203\ttotal: 101ms\tremaining: 32ms\n",
      "76:\tlearn: 0.3738334\ttotal: 103ms\tremaining: 30.6ms\n",
      "77:\tlearn: 0.3717324\ttotal: 104ms\tremaining: 29.3ms\n",
      "78:\tlearn: 0.3697667\ttotal: 105ms\tremaining: 27.9ms\n",
      "79:\tlearn: 0.3688150\ttotal: 106ms\tremaining: 26.6ms\n",
      "80:\tlearn: 0.3659629\ttotal: 108ms\tremaining: 25.2ms\n",
      "81:\tlearn: 0.3635825\ttotal: 109ms\tremaining: 23.9ms\n",
      "82:\tlearn: 0.3620269\ttotal: 110ms\tremaining: 22.6ms\n",
      "83:\tlearn: 0.3600587\ttotal: 112ms\tremaining: 21.2ms\n",
      "84:\tlearn: 0.3588464\ttotal: 113ms\tremaining: 19.9ms\n",
      "85:\tlearn: 0.3560738\ttotal: 114ms\tremaining: 18.6ms\n",
      "86:\tlearn: 0.3540266\ttotal: 116ms\tremaining: 17.3ms\n",
      "87:\tlearn: 0.3519965\ttotal: 117ms\tremaining: 16ms\n",
      "88:\tlearn: 0.3502767\ttotal: 118ms\tremaining: 14.6ms\n",
      "89:\tlearn: 0.3482537\ttotal: 120ms\tremaining: 13.3ms\n",
      "90:\tlearn: 0.3466969\ttotal: 121ms\tremaining: 12ms\n",
      "91:\tlearn: 0.3449111\ttotal: 122ms\tremaining: 10.6ms\n",
      "92:\tlearn: 0.3429223\ttotal: 124ms\tremaining: 9.3ms\n",
      "93:\tlearn: 0.3415183\ttotal: 125ms\tremaining: 7.97ms\n",
      "94:\tlearn: 0.3398895\ttotal: 126ms\tremaining: 6.64ms\n",
      "95:\tlearn: 0.3384830\ttotal: 127ms\tremaining: 5.31ms\n",
      "96:\tlearn: 0.3367521\ttotal: 129ms\tremaining: 3.98ms\n",
      "97:\tlearn: 0.3351418\ttotal: 130ms\tremaining: 2.65ms\n",
      "98:\tlearn: 0.3330892\ttotal: 131ms\tremaining: 1.32ms\n",
      "99:\tlearn: 0.3316027\ttotal: 133ms\tremaining: 0us\n",
      "evaluating model for fold number 3\n",
      "{'catboostclassifier__depth': 3, 'catboostclassifier__iterations': 100, 'catboostclassifier__learning_rate': 0.01}\n",
      "0:\tlearn: 0.6843310\ttotal: 12ms\tremaining: 1.19s\n",
      "1:\tlearn: 0.6754343\ttotal: 15.1ms\tremaining: 739ms\n",
      "2:\tlearn: 0.6672335\ttotal: 22.2ms\tremaining: 717ms\n",
      "3:\tlearn: 0.6589272\ttotal: 23.2ms\tremaining: 558ms\n",
      "4:\tlearn: 0.6508417\ttotal: 24.2ms\tremaining: 460ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:\tlearn: 0.6432553\ttotal: 32.6ms\tremaining: 511ms\n",
      "6:\tlearn: 0.6373101\ttotal: 41.4ms\tremaining: 551ms\n",
      "7:\tlearn: 0.6280618\ttotal: 46.2ms\tremaining: 531ms\n",
      "8:\tlearn: 0.6193380\ttotal: 47.3ms\tremaining: 478ms\n",
      "9:\tlearn: 0.6124351\ttotal: 48.2ms\tremaining: 434ms\n",
      "10:\tlearn: 0.6059303\ttotal: 49.2ms\tremaining: 398ms\n",
      "11:\tlearn: 0.5977625\ttotal: 50.1ms\tremaining: 368ms\n",
      "12:\tlearn: 0.5922348\ttotal: 51.1ms\tremaining: 342ms\n",
      "13:\tlearn: 0.5847601\ttotal: 52.1ms\tremaining: 320ms\n",
      "14:\tlearn: 0.5797964\ttotal: 53ms\tremaining: 300ms\n",
      "15:\tlearn: 0.5739309\ttotal: 54ms\tremaining: 283ms\n",
      "16:\tlearn: 0.5688211\ttotal: 55ms\tremaining: 268ms\n",
      "17:\tlearn: 0.5629417\ttotal: 55.9ms\tremaining: 255ms\n",
      "18:\tlearn: 0.5581913\ttotal: 56.8ms\tremaining: 242ms\n",
      "19:\tlearn: 0.5538075\ttotal: 59.4ms\tremaining: 238ms\n",
      "20:\tlearn: 0.5461016\ttotal: 60.3ms\tremaining: 227ms\n",
      "21:\tlearn: 0.5402952\ttotal: 61.3ms\tremaining: 217ms\n",
      "22:\tlearn: 0.5350369\ttotal: 62.2ms\tremaining: 208ms\n",
      "23:\tlearn: 0.5301378\ttotal: 63.2ms\tremaining: 200ms\n",
      "24:\tlearn: 0.5227813\ttotal: 64ms\tremaining: 192ms\n",
      "25:\tlearn: 0.5168821\ttotal: 65ms\tremaining: 185ms\n",
      "26:\tlearn: 0.5122225\ttotal: 66ms\tremaining: 178ms\n",
      "27:\tlearn: 0.5079600\ttotal: 67ms\tremaining: 172ms\n",
      "28:\tlearn: 0.5028439\ttotal: 67.8ms\tremaining: 166ms\n",
      "29:\tlearn: 0.4992715\ttotal: 68.8ms\tremaining: 160ms\n",
      "30:\tlearn: 0.4949854\ttotal: 77.7ms\tremaining: 173ms\n",
      "31:\tlearn: 0.4911024\ttotal: 78.7ms\tremaining: 167ms\n",
      "32:\tlearn: 0.4869399\ttotal: 79.7ms\tremaining: 162ms\n",
      "33:\tlearn: 0.4817356\ttotal: 80.5ms\tremaining: 156ms\n",
      "34:\tlearn: 0.4785065\ttotal: 81.5ms\tremaining: 151ms\n",
      "35:\tlearn: 0.4746160\ttotal: 82.4ms\tremaining: 147ms\n",
      "36:\tlearn: 0.4713706\ttotal: 83.4ms\tremaining: 142ms\n",
      "37:\tlearn: 0.4675535\ttotal: 84.3ms\tremaining: 138ms\n",
      "38:\tlearn: 0.4633142\ttotal: 85.4ms\tremaining: 134ms\n",
      "39:\tlearn: 0.4603030\ttotal: 86.4ms\tremaining: 130ms\n",
      "40:\tlearn: 0.4569765\ttotal: 87.5ms\tremaining: 126ms\n",
      "41:\tlearn: 0.4519919\ttotal: 88.7ms\tremaining: 122ms\n",
      "42:\tlearn: 0.4493066\ttotal: 89.9ms\tremaining: 119ms\n",
      "43:\tlearn: 0.4447593\ttotal: 90.9ms\tremaining: 116ms\n",
      "44:\tlearn: 0.4415683\ttotal: 91.8ms\tremaining: 112ms\n",
      "45:\tlearn: 0.4390619\ttotal: 94.6ms\tremaining: 111ms\n",
      "46:\tlearn: 0.4361831\ttotal: 95.6ms\tremaining: 108ms\n",
      "47:\tlearn: 0.4339329\ttotal: 96.6ms\tremaining: 105ms\n",
      "48:\tlearn: 0.4306045\ttotal: 97.5ms\tremaining: 102ms\n",
      "49:\tlearn: 0.4269904\ttotal: 98.4ms\tremaining: 98.4ms\n",
      "50:\tlearn: 0.4250411\ttotal: 99.3ms\tremaining: 95.4ms\n",
      "51:\tlearn: 0.4225732\ttotal: 100ms\tremaining: 92.5ms\n",
      "52:\tlearn: 0.4202898\ttotal: 101ms\tremaining: 89.6ms\n",
      "53:\tlearn: 0.4176433\ttotal: 102ms\tremaining: 86.8ms\n",
      "54:\tlearn: 0.4143144\ttotal: 103ms\tremaining: 84.2ms\n",
      "55:\tlearn: 0.4119930\ttotal: 104ms\tremaining: 81.5ms\n",
      "56:\tlearn: 0.4082287\ttotal: 105ms\tremaining: 79ms\n",
      "57:\tlearn: 0.4050153\ttotal: 106ms\tremaining: 76.5ms\n",
      "58:\tlearn: 0.4030342\ttotal: 107ms\tremaining: 74.1ms\n",
      "59:\tlearn: 0.4014304\ttotal: 107ms\tremaining: 71.6ms\n",
      "60:\tlearn: 0.3993400\ttotal: 108ms\tremaining: 69.3ms\n",
      "61:\tlearn: 0.3975774\ttotal: 109ms\tremaining: 66.9ms\n",
      "62:\tlearn: 0.3949767\ttotal: 110ms\tremaining: 64.6ms\n",
      "63:\tlearn: 0.3929466\ttotal: 111ms\tremaining: 62.4ms\n",
      "64:\tlearn: 0.3899223\ttotal: 112ms\tremaining: 60.2ms\n",
      "65:\tlearn: 0.3868940\ttotal: 113ms\tremaining: 58ms\n",
      "66:\tlearn: 0.3844646\ttotal: 113ms\tremaining: 55.8ms\n",
      "67:\tlearn: 0.3816078\ttotal: 114ms\tremaining: 53.7ms\n",
      "68:\tlearn: 0.3790671\ttotal: 115ms\tremaining: 51.6ms\n",
      "69:\tlearn: 0.3771436\ttotal: 116ms\tremaining: 49.6ms\n",
      "70:\tlearn: 0.3753825\ttotal: 117ms\tremaining: 47.6ms\n",
      "71:\tlearn: 0.3736206\ttotal: 117ms\tremaining: 45.7ms\n",
      "72:\tlearn: 0.3716045\ttotal: 118ms\tremaining: 43.7ms\n",
      "73:\tlearn: 0.3697373\ttotal: 119ms\tremaining: 41.8ms\n",
      "74:\tlearn: 0.3673297\ttotal: 120ms\tremaining: 40ms\n",
      "75:\tlearn: 0.3649292\ttotal: 121ms\tremaining: 38.2ms\n",
      "76:\tlearn: 0.3631061\ttotal: 122ms\tremaining: 36.4ms\n",
      "77:\tlearn: 0.3617891\ttotal: 123ms\tremaining: 34.6ms\n",
      "78:\tlearn: 0.3595411\ttotal: 124ms\tremaining: 32.9ms\n",
      "79:\tlearn: 0.3574623\ttotal: 125ms\tremaining: 31.2ms\n",
      "80:\tlearn: 0.3550106\ttotal: 126ms\tremaining: 29.5ms\n",
      "81:\tlearn: 0.3528020\ttotal: 127ms\tremaining: 27.8ms\n",
      "82:\tlearn: 0.3514531\ttotal: 127ms\tremaining: 26.1ms\n",
      "83:\tlearn: 0.3490624\ttotal: 128ms\tremaining: 24.4ms\n",
      "84:\tlearn: 0.3480850\ttotal: 129ms\tremaining: 22.8ms\n",
      "85:\tlearn: 0.3458653\ttotal: 130ms\tremaining: 21.2ms\n",
      "86:\tlearn: 0.3443673\ttotal: 131ms\tremaining: 19.5ms\n",
      "87:\tlearn: 0.3426720\ttotal: 132ms\tremaining: 18ms\n",
      "88:\tlearn: 0.3411534\ttotal: 133ms\tremaining: 16.4ms\n",
      "89:\tlearn: 0.3398089\ttotal: 134ms\tremaining: 14.8ms\n",
      "90:\tlearn: 0.3377392\ttotal: 135ms\tremaining: 13.3ms\n",
      "91:\tlearn: 0.3351535\ttotal: 135ms\tremaining: 11.8ms\n",
      "92:\tlearn: 0.3343037\ttotal: 136ms\tremaining: 10.3ms\n",
      "93:\tlearn: 0.3321730\ttotal: 137ms\tremaining: 8.76ms\n",
      "94:\tlearn: 0.3307851\ttotal: 138ms\tremaining: 7.27ms\n",
      "95:\tlearn: 0.3293225\ttotal: 139ms\tremaining: 5.79ms\n",
      "96:\tlearn: 0.3277952\ttotal: 140ms\tremaining: 4.33ms\n",
      "97:\tlearn: 0.3264375\ttotal: 141ms\tremaining: 2.87ms\n",
      "98:\tlearn: 0.3247682\ttotal: 142ms\tremaining: 1.43ms\n",
      "99:\tlearn: 0.3236235\ttotal: 143ms\tremaining: 0us\n",
      "parameters: {'catboostclassifier__depth': 3, 'catboostclassifier__iterations': 100, 'catboostclassifier__learning_rate': 0.01}\n",
      "AUC score: 0.798913043478261\n",
      "finding best model parameters for fold number 4\n",
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n",
      "0:\tlearn: 0.6626208\ttotal: 6.05ms\tremaining: 297ms\n",
      "1:\tlearn: 0.6299106\ttotal: 9.16ms\tremaining: 220ms\n",
      "2:\tlearn: 0.5959882\ttotal: 10.5ms\tremaining: 164ms\n",
      "3:\tlearn: 0.5696305\ttotal: 11.8ms\tremaining: 136ms\n",
      "4:\tlearn: 0.5450134\ttotal: 13.1ms\tremaining: 118ms\n",
      "5:\tlearn: 0.5283050\ttotal: 14.4ms\tremaining: 105ms\n",
      "6:\tlearn: 0.5082099\ttotal: 15.6ms\tremaining: 95.8ms\n",
      "7:\tlearn: 0.4913198\ttotal: 16.8ms\tremaining: 88.4ms\n",
      "8:\tlearn: 0.4703194\ttotal: 18.1ms\tremaining: 82.4ms\n",
      "9:\tlearn: 0.4530253\ttotal: 19.4ms\tremaining: 77.5ms\n",
      "10:\tlearn: 0.4395481\ttotal: 20.7ms\tremaining: 73.3ms\n",
      "11:\tlearn: 0.4310258\ttotal: 22ms\tremaining: 69.5ms\n",
      "12:\tlearn: 0.4165196\ttotal: 23.3ms\tremaining: 66.2ms\n",
      "13:\tlearn: 0.4033481\ttotal: 24.5ms\tremaining: 63ms\n",
      "14:\tlearn: 0.3891282\ttotal: 25.8ms\tremaining: 60.1ms\n",
      "15:\tlearn: 0.3825906\ttotal: 27ms\tremaining: 57.4ms\n",
      "16:\tlearn: 0.3739835\ttotal: 28.4ms\tremaining: 55ms\n",
      "17:\tlearn: 0.3676428\ttotal: 29.7ms\tremaining: 52.7ms\n",
      "18:\tlearn: 0.3580682\ttotal: 30.9ms\tremaining: 50.4ms\n",
      "19:\tlearn: 0.3454938\ttotal: 32.1ms\tremaining: 48.2ms\n",
      "20:\tlearn: 0.3383634\ttotal: 33.4ms\tremaining: 46.1ms\n",
      "21:\tlearn: 0.3317465\ttotal: 34.7ms\tremaining: 44.1ms\n",
      "22:\tlearn: 0.3264480\ttotal: 36ms\tremaining: 42.2ms\n",
      "23:\tlearn: 0.3174753\ttotal: 37.2ms\tremaining: 40.3ms\n",
      "24:\tlearn: 0.3107711\ttotal: 38.8ms\tremaining: 38.8ms\n",
      "25:\tlearn: 0.3038126\ttotal: 40.2ms\tremaining: 37.1ms\n",
      "26:\tlearn: 0.2975942\ttotal: 41.5ms\tremaining: 35.3ms\n",
      "27:\tlearn: 0.2895246\ttotal: 42.7ms\tremaining: 33.6ms\n",
      "28:\tlearn: 0.2851944\ttotal: 44ms\tremaining: 31.9ms\n",
      "29:\tlearn: 0.2799900\ttotal: 45.2ms\tremaining: 30.2ms\n",
      "30:\tlearn: 0.2745640\ttotal: 46.7ms\tremaining: 28.6ms\n",
      "31:\tlearn: 0.2697465\ttotal: 47.9ms\tremaining: 27ms\n",
      "32:\tlearn: 0.2648160\ttotal: 49.2ms\tremaining: 25.4ms\n",
      "33:\tlearn: 0.2586372\ttotal: 50.5ms\tremaining: 23.7ms\n",
      "34:\tlearn: 0.2513359\ttotal: 51.8ms\tremaining: 22.2ms\n",
      "35:\tlearn: 0.2449679\ttotal: 53.1ms\tremaining: 20.6ms\n",
      "36:\tlearn: 0.2390165\ttotal: 54.3ms\tremaining: 19.1ms\n",
      "37:\tlearn: 0.2348149\ttotal: 55.6ms\tremaining: 17.5ms\n",
      "38:\tlearn: 0.2302852\ttotal: 56.9ms\tremaining: 16ms\n",
      "39:\tlearn: 0.2272286\ttotal: 58.1ms\tremaining: 14.5ms\n",
      "40:\tlearn: 0.2238940\ttotal: 59.5ms\tremaining: 13.1ms\n",
      "41:\tlearn: 0.2211664\ttotal: 60.8ms\tremaining: 11.6ms\n",
      "42:\tlearn: 0.2163774\ttotal: 62.1ms\tremaining: 10.1ms\n",
      "43:\tlearn: 0.2129387\ttotal: 63.3ms\tremaining: 8.64ms\n",
      "44:\tlearn: 0.2098762\ttotal: 64.6ms\tremaining: 7.18ms\n",
      "45:\tlearn: 0.2074482\ttotal: 65.8ms\tremaining: 5.72ms\n",
      "46:\tlearn: 0.2041056\ttotal: 67.1ms\tremaining: 4.28ms\n",
      "47:\tlearn: 0.2018073\ttotal: 68.3ms\tremaining: 2.85ms\n",
      "48:\tlearn: 0.1997213\ttotal: 69.6ms\tremaining: 1.42ms\n",
      "49:\tlearn: 0.1966286\ttotal: 70.8ms\tremaining: 0us\n",
      "evaluating model for fold number 4\n",
      "{'catboostclassifier__depth': 3, 'catboostclassifier__iterations': 50, 'catboostclassifier__learning_rate': 0.05}\n",
      "0:\tlearn: 0.6495066\ttotal: 7.54ms\tremaining: 370ms\n",
      "1:\tlearn: 0.6120638\ttotal: 10.6ms\tremaining: 255ms\n",
      "2:\tlearn: 0.5791672\ttotal: 14ms\tremaining: 220ms\n",
      "3:\tlearn: 0.5528952\ttotal: 15.3ms\tremaining: 175ms\n",
      "4:\tlearn: 0.5284559\ttotal: 17.1ms\tremaining: 154ms\n",
      "5:\tlearn: 0.5098829\ttotal: 18.2ms\tremaining: 133ms\n",
      "6:\tlearn: 0.4948607\ttotal: 21.2ms\tremaining: 130ms\n",
      "7:\tlearn: 0.4732988\ttotal: 22.3ms\tremaining: 117ms\n",
      "8:\tlearn: 0.4563634\ttotal: 23.3ms\tremaining: 106ms\n",
      "9:\tlearn: 0.4409037\ttotal: 24.3ms\tremaining: 97.1ms\n",
      "10:\tlearn: 0.4240625\ttotal: 25.2ms\tremaining: 89.5ms\n",
      "11:\tlearn: 0.4116609\ttotal: 26.2ms\tremaining: 82.9ms\n",
      "12:\tlearn: 0.4019853\ttotal: 27.1ms\tremaining: 77.3ms\n",
      "13:\tlearn: 0.3901986\ttotal: 28.1ms\tremaining: 72.3ms\n",
      "14:\tlearn: 0.3825502\ttotal: 29ms\tremaining: 67.7ms\n",
      "15:\tlearn: 0.3720044\ttotal: 30ms\tremaining: 63.7ms\n",
      "16:\tlearn: 0.3645227\ttotal: 30.9ms\tremaining: 60ms\n",
      "17:\tlearn: 0.3560335\ttotal: 31.8ms\tremaining: 56.6ms\n",
      "18:\tlearn: 0.3476050\ttotal: 32.7ms\tremaining: 53.4ms\n",
      "19:\tlearn: 0.3408090\ttotal: 33.8ms\tremaining: 50.6ms\n",
      "20:\tlearn: 0.3342197\ttotal: 34.8ms\tremaining: 48ms\n",
      "21:\tlearn: 0.3263887\ttotal: 35.7ms\tremaining: 45.5ms\n",
      "22:\tlearn: 0.3203108\ttotal: 36.6ms\tremaining: 43ms\n",
      "23:\tlearn: 0.3126312\ttotal: 37.5ms\tremaining: 40.6ms\n",
      "24:\tlearn: 0.3068835\ttotal: 38.5ms\tremaining: 38.5ms\n",
      "25:\tlearn: 0.3001871\ttotal: 39.4ms\tremaining: 36.4ms\n",
      "26:\tlearn: 0.2937368\ttotal: 40.3ms\tremaining: 34.4ms\n",
      "27:\tlearn: 0.2894848\ttotal: 41.3ms\tremaining: 32.4ms\n",
      "28:\tlearn: 0.2872200\ttotal: 42.3ms\tremaining: 30.6ms\n",
      "29:\tlearn: 0.2831297\ttotal: 43.2ms\tremaining: 28.8ms\n",
      "30:\tlearn: 0.2776209\ttotal: 47.6ms\tremaining: 29.2ms\n",
      "31:\tlearn: 0.2739068\ttotal: 48.6ms\tremaining: 27.4ms\n",
      "32:\tlearn: 0.2676706\ttotal: 49.5ms\tremaining: 25.5ms\n",
      "33:\tlearn: 0.2637918\ttotal: 50.4ms\tremaining: 23.7ms\n",
      "34:\tlearn: 0.2601159\ttotal: 51.5ms\tremaining: 22.1ms\n",
      "35:\tlearn: 0.2582772\ttotal: 54.5ms\tremaining: 21.2ms\n",
      "36:\tlearn: 0.2563623\ttotal: 55.6ms\tremaining: 19.5ms\n",
      "37:\tlearn: 0.2544739\ttotal: 56.6ms\tremaining: 17.9ms\n",
      "38:\tlearn: 0.2519345\ttotal: 57.5ms\tremaining: 16.2ms\n",
      "39:\tlearn: 0.2494701\ttotal: 58.5ms\tremaining: 14.6ms\n",
      "40:\tlearn: 0.2454527\ttotal: 59.4ms\tremaining: 13ms\n",
      "41:\tlearn: 0.2416951\ttotal: 60.3ms\tremaining: 11.5ms\n",
      "42:\tlearn: 0.2377954\ttotal: 61.3ms\tremaining: 9.97ms\n",
      "43:\tlearn: 0.2350197\ttotal: 62.2ms\tremaining: 8.48ms\n",
      "44:\tlearn: 0.2328118\ttotal: 63.1ms\tremaining: 7.01ms\n",
      "45:\tlearn: 0.2305285\ttotal: 64ms\tremaining: 5.56ms\n",
      "46:\tlearn: 0.2289846\ttotal: 66.8ms\tremaining: 4.26ms\n",
      "47:\tlearn: 0.2267874\ttotal: 67.9ms\tremaining: 2.83ms\n",
      "48:\tlearn: 0.2244566\ttotal: 68.8ms\tremaining: 1.4ms\n",
      "49:\tlearn: 0.2208871\ttotal: 69.7ms\tremaining: 0us\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'catboostclassifier__depth': 3, 'catboostclassifier__iterations': 50, 'catboostclassifier__learning_rate': 0.05}\n",
      "AUC score: 0.8416149068322981\n",
      "finding best model parameters for fold number 5\n",
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n",
      "0:\tlearn: 0.6578717\ttotal: 9.06ms\tremaining: 172ms\n",
      "1:\tlearn: 0.6309444\ttotal: 12.1ms\tremaining: 108ms\n",
      "2:\tlearn: 0.6035201\ttotal: 13.3ms\tremaining: 75.4ms\n",
      "3:\tlearn: 0.5834612\ttotal: 14.5ms\tremaining: 58ms\n",
      "4:\tlearn: 0.5611351\ttotal: 15.7ms\tremaining: 47.1ms\n",
      "5:\tlearn: 0.5368898\ttotal: 16.9ms\tremaining: 39.4ms\n",
      "6:\tlearn: 0.5247337\ttotal: 18.1ms\tremaining: 33.5ms\n",
      "7:\tlearn: 0.5050687\ttotal: 19.2ms\tremaining: 28.8ms\n",
      "8:\tlearn: 0.4973934\ttotal: 20.4ms\tremaining: 25ms\n",
      "9:\tlearn: 0.4773655\ttotal: 21.7ms\tremaining: 21.7ms\n",
      "10:\tlearn: 0.4623923\ttotal: 22.9ms\tremaining: 18.7ms\n",
      "11:\tlearn: 0.4472013\ttotal: 24.1ms\tremaining: 16.1ms\n",
      "12:\tlearn: 0.4357787\ttotal: 25.3ms\tremaining: 13.6ms\n",
      "13:\tlearn: 0.4224886\ttotal: 26.6ms\tremaining: 11.4ms\n",
      "14:\tlearn: 0.4126749\ttotal: 27.9ms\tremaining: 9.32ms\n",
      "15:\tlearn: 0.3975154\ttotal: 29.3ms\tremaining: 7.32ms\n",
      "16:\tlearn: 0.3873328\ttotal: 30.5ms\tremaining: 5.38ms\n",
      "17:\tlearn: 0.3764519\ttotal: 31.8ms\tremaining: 3.53ms\n",
      "18:\tlearn: 0.3670048\ttotal: 33.1ms\tremaining: 1.74ms\n",
      "19:\tlearn: 0.3586084\ttotal: 34.4ms\tremaining: 0us\n",
      "evaluating model for fold number 5\n",
      "{'catboostclassifier__depth': 3, 'catboostclassifier__iterations': 20, 'catboostclassifier__learning_rate': 0.05}\n",
      "0:\tlearn: 0.6567655\ttotal: 3.41ms\tremaining: 64.8ms\n",
      "1:\tlearn: 0.6160608\ttotal: 6.44ms\tremaining: 57.9ms\n",
      "2:\tlearn: 0.5859329\ttotal: 7.47ms\tremaining: 42.3ms\n",
      "3:\tlearn: 0.5598498\ttotal: 8.53ms\tremaining: 34.1ms\n",
      "4:\tlearn: 0.5351174\ttotal: 9.5ms\tremaining: 28.5ms\n",
      "5:\tlearn: 0.5123707\ttotal: 10.5ms\tremaining: 24.4ms\n",
      "6:\tlearn: 0.4974265\ttotal: 11.5ms\tremaining: 21.4ms\n",
      "7:\tlearn: 0.4729723\ttotal: 12.5ms\tremaining: 18.8ms\n",
      "8:\tlearn: 0.4532518\ttotal: 15.4ms\tremaining: 18.8ms\n",
      "9:\tlearn: 0.4346679\ttotal: 16.5ms\tremaining: 16.5ms\n",
      "10:\tlearn: 0.4180121\ttotal: 17.5ms\tremaining: 14.3ms\n",
      "11:\tlearn: 0.4042106\ttotal: 18.5ms\tremaining: 12.3ms\n",
      "12:\tlearn: 0.3924754\ttotal: 19.4ms\tremaining: 10.5ms\n",
      "13:\tlearn: 0.3782350\ttotal: 20.4ms\tremaining: 8.72ms\n",
      "14:\tlearn: 0.3705760\ttotal: 21.6ms\tremaining: 7.21ms\n",
      "15:\tlearn: 0.3579795\ttotal: 22.8ms\tremaining: 5.69ms\n",
      "16:\tlearn: 0.3490242\ttotal: 23.7ms\tremaining: 4.19ms\n",
      "17:\tlearn: 0.3430306\ttotal: 24.7ms\tremaining: 2.74ms\n",
      "18:\tlearn: 0.3354993\ttotal: 25.7ms\tremaining: 1.35ms\n",
      "19:\tlearn: 0.3299113\ttotal: 26.6ms\tremaining: 0us\n",
      "parameters: {'catboostclassifier__depth': 3, 'catboostclassifier__iterations': 20, 'catboostclassifier__learning_rate': 0.05}\n",
      "AUC score: 0.7981366459627329\n"
     ]
    }
   ],
   "source": [
    "# # tune catboost\n",
    "\n",
    "# fname = 'experiments/cb_tuning' + '_' + str(datetime.now().year) + '_' + str(datetime.now().month) \\\n",
    "#     + '_' + str(datetime.now().day) + '_' + str(datetime.now().hour) + '_' + \\\n",
    "#     str(datetime.now().minute) + '.txt'\n",
    "\n",
    "# eval_cb_params(fname = fname, param_grid = cb_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bef0b4",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning - Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce7e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to build a keras DNN model\n",
    "\n",
    "def build_model(input_shape = X_train.shape[1], layer_size_factor = 3, num_hidden_layers = 1, dropout_rate = 0):\n",
    "    \n",
    "    inputs = Input(shape = input_shape)\n",
    "    x = Dense(np.power(2, layer_size_factor), activation = 'relu')(inputs)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    if num_hidden_layers > 1:\n",
    "        for i in range(num_hidden_layers - 1):\n",
    "            x = Dense(np.power(2, layer_size_factor), activation = 'relu')(x)\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics = [AUC()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c69a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define param grid for grid search\n",
    "\n",
    "sizes = np.arange(3, 9)\n",
    "layers = np.arange(1, 4) \n",
    "rates = np.arange(0, 0.6, 0.1)\n",
    "batch_size = [40, 60, 80, 100]\n",
    "epochs = [10, 50, 100] \n",
    "\n",
    "keras_grid = dict(kerasclassifier__layer_size_factor = sizes, \n",
    "                 kerasclassifier__num_hidden_layers = layers, \n",
    "                 kerasclassifier__dropout_rate = rates, \n",
    "                 kerasclassifier__batch_size = batch_size, \n",
    "                 kerasclassifier__epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "000c7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested CV to find best parameters for DNN model\n",
    "\n",
    "def eval_keras_params(fname, param_grid):\n",
    "\n",
    "    results = []\n",
    "    result_file = open(fname, 'a')\n",
    "\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "    fold_no = 1\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "\n",
    "        X_train_split, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_split, y_test = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        # find best model params\n",
    "        print('finding best model parameters for fold number {}'.format(fold_no))\n",
    "        model = make_pipeline(\n",
    "            SMOTE(),\n",
    "            KerasClassifier(build_fn = build_model, input_shape = X_train_split.shape[1]) # wrapper for keras model\n",
    "        )\n",
    "        grid = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs = -1, cv = 5, error_score = 'raise', \n",
    "                           scoring = 'roc_auc', verbose = 2)\n",
    "        grid_result = grid.fit(X_train_split, y_train_split)\n",
    "    \n",
    "        # evaluate best model params on outer fold\n",
    "        print('evaluating model for fold number {}'.format(fold_no))\n",
    "        best_params = grid_result.best_params_\n",
    "        best_model = make_pipeline_sk(KerasClassifier(build_fn = build_model, input_shape = X_train_split.shape[1]))\n",
    "        best_model.set_params(**best_params)\n",
    "        best_model.fit(X_train_split, y_train_split)\n",
    "        score = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "        results.append((best_params, score))\n",
    "        print('parameters: {}'.format(str(best_params)))\n",
    "        print('AUC score: {}'.format(score))\n",
    "        fold_no += 1\n",
    "\n",
    "    results.sort(key = lambda x:x[-1], reverse = True)\n",
    "    result_file.write('\\nAUC scores:\\n\\n')\n",
    "    \n",
    "    # write score summary to text file\n",
    "    for best_params, score in results:\n",
    "        result_file.write(str(best_params) + '\\t' + str(score) + '\\n')\n",
    "    result_file.close()\n",
    "    \n",
    "    # write scores to dataframe:\n",
    "    df = pd.DataFrame(results, columns = ['params', 'score'])\n",
    "    df.to_pickle('experiments/hyperparameter_tuning_results_keras.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25268306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding best model parameters for fold number 1\n",
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 2ms/step - loss: 0.8499 - auc_10: 0.5251\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.8320 - auc_10: 0.4827\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7479 - auc_10: 0.5272\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7151 - auc_10: 0.5690\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7294 - auc_10: 0.5265\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7159 - auc_10: 0.5438\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7222 - auc_10: 0.5278\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7102 - auc_10: 0.5336\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6977 - auc_10: 0.5618\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6959 - auc_10: 0.5318\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7078 - auc_10: 0.5155\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6660 - auc_10: 0.6124\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6638 - auc_10: 0.6252\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6584 - auc_10: 0.6184\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6624 - auc_10: 0.6176\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6567 - auc_10: 0.6320\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6640 - auc_10: 0.6100\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6649 - auc_10: 0.6047\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6587 - auc_10: 0.6140\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6258 - auc_10: 0.7190\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6230 - auc_10: 0.7157\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6257 - auc_10: 0.6907\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6223 - auc_10: 0.6816\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6019 - auc_10: 0.7319\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6262 - auc_10: 0.6883\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6319 - auc_10: 0.6771\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5972 - auc_10: 0.7460\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6131 - auc_10: 0.6999\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6025 - auc_10: 0.7194\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5889 - auc_10: 0.7358\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6118 - auc_10: 0.7129\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5908 - auc_10: 0.7541\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5809 - auc_10: 0.7455\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5594 - auc_10: 0.7934\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5407 - auc_10: 0.8030\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5506 - auc_10: 0.8047\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5466 - auc_10: 0.8045\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5490 - auc_10: 0.7932\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5452 - auc_10: 0.7955\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5277 - auc_10: 0.8296\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5552 - auc_10: 0.7836\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5332 - auc_10: 0.8241\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5188 - auc_10: 0.8295\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5138 - auc_10: 0.8393\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5102 - auc_10: 0.8389\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5168 - auc_10: 0.8208\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4743 - auc_10: 0.8646\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5197 - auc_10: 0.8347\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4977 - auc_10: 0.8411\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4662 - auc_10: 0.8685\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4870 - auc_10: 0.8567\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4658 - auc_10: 0.8833\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4530 - auc_10: 0.8894\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4656 - auc_10: 0.8704\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4522 - auc_10: 0.8855\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4429 - auc_10: 0.8666\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4404 - auc_10: 0.8801\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4549 - auc_10: 0.8769\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3954 - auc_10: 0.9102\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4193 - auc_10: 0.8883\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4545 - auc_10: 0.8713\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4046 - auc_10: 0.9071\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4019 - auc_10: 0.8997\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3909 - auc_10: 0.9154\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3892 - auc_10: 0.9048\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4198 - auc_10: 0.8981\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4020 - auc_10: 0.9076\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3807 - auc_10: 0.9126\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4191 - auc_10: 0.8841\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3442 - auc_10: 0.9523\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3591 - auc_10: 0.9307\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3832 - auc_10: 0.9167\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3407 - auc_10: 0.9398\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3416 - auc_10: 0.9328\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3497 - auc_10: 0.9263\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3746 - auc_10: 0.9056\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3360 - auc_10: 0.9307\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3571 - auc_10: 0.9124\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3262 - auc_10: 0.9409\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3279 - auc_10: 0.9349\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3460 - auc_10: 0.9344\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3216 - auc_10: 0.9381\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3430 - auc_10: 0.9285\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3226 - auc_10: 0.9376\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3171 - auc_10: 0.9418\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3203 - auc_10: 0.9415\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3063 - auc_10: 0.9356\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3127 - auc_10: 0.9393\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2859 - auc_10: 0.9574\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3055 - auc_10: 0.9444\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2764 - auc_10: 0.9562\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2556 - auc_10: 0.9678\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2549 - auc_10: 0.9637\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2905 - auc_10: 0.9454\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2922 - auc_10: 0.9522\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2625 - auc_10: 0.9585\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2631 - auc_10: 0.9614\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2395 - auc_10: 0.9629\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2581 - auc_10: 0.9636\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2397 - auc_10: 0.9630\n",
      "evaluating model for fold number 1\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 2ms/step - loss: 0.6031 - auc_11: 0.4916\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5328 - auc_11: 0.5994\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5766 - auc_11: 0.5149\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5210 - auc_11: 0.5300\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5638 - auc_11: 0.4824\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5399 - auc_11: 0.4961\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4619 - auc_11: 0.6459\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4902 - auc_11: 0.5641\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4967 - auc_11: 0.5557\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5573 - auc_11: 0.4432\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5028 - auc_11: 0.5077\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4453 - auc_11: 0.6388\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4885 - auc_11: 0.5208\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4534 - auc_11: 0.5832\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4915 - auc_11: 0.5191\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4853 - auc_11: 0.5382\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4891 - auc_11: 0.5051\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4991 - auc_11: 0.4892\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4559 - auc_11: 0.5773\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4549 - auc_11: 0.5558\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4611 - auc_11: 0.5735\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4248 - auc_11: 0.6275\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4119 - auc_11: 0.6476\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4210 - auc_11: 0.6345\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4353 - auc_11: 0.6245\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4151 - auc_11: 0.6439\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4240 - auc_11: 0.6253\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3864 - auc_11: 0.7019\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4375 - auc_11: 0.5694\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4290 - auc_11: 0.6125\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4289 - auc_11: 0.6538\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4106 - auc_11: 0.6556\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4118 - auc_11: 0.6503\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4093 - auc_11: 0.6393\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4312 - auc_11: 0.5975\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4298 - auc_11: 0.6189\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3893 - auc_11: 0.6662\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3807 - auc_11: 0.6798\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4158 - auc_11: 0.6496\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4149 - auc_11: 0.6466\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4245 - auc_11: 0.6143\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3810 - auc_11: 0.6841\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4014 - auc_11: 0.6595\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3744 - auc_11: 0.7206\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3676 - auc_11: 0.7052\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3426 - auc_11: 0.7523\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3764 - auc_11: 0.7222\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3507 - auc_11: 0.7548\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3807 - auc_11: 0.6907\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3864 - auc_11: 0.6861\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3545 - auc_11: 0.7332\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3312 - auc_11: 0.7836\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3454 - auc_11: 0.7715\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3803 - auc_11: 0.6797\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3831 - auc_11: 0.6845\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3860 - auc_11: 0.7190\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3338 - auc_11: 0.7806\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3537 - auc_11: 0.7277\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3594 - auc_11: 0.7091\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3280 - auc_11: 0.7791\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3292 - auc_11: 0.7800\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3255 - auc_11: 0.7924\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3112 - auc_11: 0.8208\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3275 - auc_11: 0.7763\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3143 - auc_11: 0.8201\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3138 - auc_11: 0.8202\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3135 - auc_11: 0.8090\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3355 - auc_11: 0.7894\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3040 - auc_11: 0.8271\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3243 - auc_11: 0.7892\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3353 - auc_11: 0.7813\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3178 - auc_11: 0.7967\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3168 - auc_11: 0.8132\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3378 - auc_11: 0.8044\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3183 - auc_11: 0.7978\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3065 - auc_11: 0.8339\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2992 - auc_11: 0.8419\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3218 - auc_11: 0.8145\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3153 - auc_11: 0.8131\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2884 - auc_11: 0.8481\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2869 - auc_11: 0.8648\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2899 - auc_11: 0.8571\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2976 - auc_11: 0.8676\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3107 - auc_11: 0.8190\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2768 - auc_11: 0.8672\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2885 - auc_11: 0.8490\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2623 - auc_11: 0.9019\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3130 - auc_11: 0.8250\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2923 - auc_11: 0.8618\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2724 - auc_11: 0.8729\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2892 - auc_11: 0.8480\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2785 - auc_11: 0.8751\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2541 - auc_11: 0.9072\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2893 - auc_11: 0.8671\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2414 - auc_11: 0.9120\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2815 - auc_11: 0.8710\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2638 - auc_11: 0.8893\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2549 - auc_11: 0.9020\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2705 - auc_11: 0.8887\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2444 - auc_11: 0.9093\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc13ad6dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "parameters: {'kerasclassifier__batch_size': 100, 'kerasclassifier__dropout_rate': 0.30000000000000004, 'kerasclassifier__epochs': 100, 'kerasclassifier__layer_size_factor': 3, 'kerasclassifier__num_hidden_layers': 3}\n",
      "AUC score: 0.6768867924528301\n",
      "finding best model parameters for fold number 2\n",
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 1ms/step - loss: 0.7191 - auc_12: 0.6563\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6359 - auc_12: 0.7524\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5959 - auc_12: 0.8165\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5671 - auc_12: 0.8577\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5387 - auc_12: 0.8869\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5091 - auc_12: 0.9082\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4832 - auc_12: 0.9251\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4622 - auc_12: 0.9336\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4435 - auc_12: 0.9384\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4237 - auc_12: 0.9426\n",
      "evaluating model for fold number 2\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 2ms/step - loss: 0.6236 - auc_13: 0.4540\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5299 - auc_13: 0.4953\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4660 - auc_13: 0.5248\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4242 - auc_13: 0.5409\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3948 - auc_13: 0.5749\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3793 - auc_13: 0.6156\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3696 - auc_13: 0.6496\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3617 - auc_13: 0.6885\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3531 - auc_13: 0.7260\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3434 - auc_13: 0.7623\n",
      "parameters: {'kerasclassifier__batch_size': 100, 'kerasclassifier__dropout_rate': 0.0, 'kerasclassifier__epochs': 10, 'kerasclassifier__layer_size_factor': 5, 'kerasclassifier__num_hidden_layers': 1}\n",
      "AUC score: 0.6202830188679245\n",
      "finding best model parameters for fold number 3\n",
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 2ms/step - loss: 0.8649 - auc_14: 0.5742\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7828 - auc_14: 0.6233\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7445 - auc_14: 0.5979\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7144 - auc_14: 0.6353\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7155 - auc_14: 0.6444\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7065 - auc_14: 0.6518\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6802 - auc_14: 0.6781\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6653 - auc_14: 0.6924\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6763 - auc_14: 0.6779\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6385 - auc_14: 0.7303\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6532 - auc_14: 0.7391\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6411 - auc_14: 0.7577\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6353 - auc_14: 0.7623\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6209 - auc_14: 0.7942\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6034 - auc_14: 0.8303\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5850 - auc_14: 0.8443\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5893 - auc_14: 0.8379\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5883 - auc_14: 0.8398\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5670 - auc_14: 0.8617\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5845 - auc_14: 0.8323\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5510 - auc_14: 0.8779\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5591 - auc_14: 0.8605\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5625 - auc_14: 0.8673\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5412 - auc_14: 0.8747\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5430 - auc_14: 0.8859\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5387 - auc_14: 0.8871\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5489 - auc_14: 0.8644\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5305 - auc_14: 0.8846\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5222 - auc_14: 0.8880\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5126 - auc_14: 0.9043\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5206 - auc_14: 0.8881\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5285 - auc_14: 0.8893\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5035 - auc_14: 0.9007\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4975 - auc_14: 0.8989\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5072 - auc_14: 0.8857\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5036 - auc_14: 0.8976\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4916 - auc_14: 0.9120\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4908 - auc_14: 0.9035\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4955 - auc_14: 0.9115\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4746 - auc_14: 0.9280\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4822 - auc_14: 0.9193\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4732 - auc_14: 0.9075\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4725 - auc_14: 0.9151\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4584 - auc_14: 0.9269\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4562 - auc_14: 0.9252\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4614 - auc_14: 0.9156\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4674 - auc_14: 0.9117\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4455 - auc_14: 0.9340\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4498 - auc_14: 0.9309\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4493 - auc_14: 0.9248\n",
      "evaluating model for fold number 3\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 2ms/step - loss: 1.6504 - auc_15: 0.5515\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.4674 - auc_15: 0.5520\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.3407 - auc_15: 0.6304\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.1284 - auc_15: 0.5739\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9598 - auc_15: 0.5961\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.9332 - auc_15: 0.6131\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.8413 - auc_15: 0.6038\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7592 - auc_15: 0.6282\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6999 - auc_15: 0.6511\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6671 - auc_15: 0.5741\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6116 - auc_15: 0.6042\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6076 - auc_15: 0.5508\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5777 - auc_15: 0.5361\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5646 - auc_15: 0.5860\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5232 - auc_15: 0.5587\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5230 - auc_15: 0.5264\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5203 - auc_15: 0.5712\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4978 - auc_15: 0.5285\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4539 - auc_15: 0.6246\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4517 - auc_15: 0.5914\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4714 - auc_15: 0.5533\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4483 - auc_15: 0.5797\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4397 - auc_15: 0.6547\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4083 - auc_15: 0.6281\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4543 - auc_15: 0.5846\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4568 - auc_15: 0.5443\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4122 - auc_15: 0.6491\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.4352 - auc_15: 0.5689\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4689 - auc_15: 0.5320\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3981 - auc_15: 0.6519\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4159 - auc_15: 0.6579\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4157 - auc_15: 0.6208\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4488 - auc_15: 0.5560\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4224 - auc_15: 0.6192\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4097 - auc_15: 0.6364\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4071 - auc_15: 0.6177\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3725 - auc_15: 0.7161\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3588 - auc_15: 0.7484\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4198 - auc_15: 0.6192\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3850 - auc_15: 0.6845\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3891 - auc_15: 0.6851\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3968 - auc_15: 0.6834\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3673 - auc_15: 0.7228\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3951 - auc_15: 0.6586\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3727 - auc_15: 0.7203\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4056 - auc_15: 0.6649\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3686 - auc_15: 0.7144\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3949 - auc_15: 0.6795\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3839 - auc_15: 0.7117\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4090 - auc_15: 0.6686\n",
      "parameters: {'kerasclassifier__batch_size': 60, 'kerasclassifier__dropout_rate': 0.4, 'kerasclassifier__epochs': 50, 'kerasclassifier__layer_size_factor': 3, 'kerasclassifier__num_hidden_layers': 1}\n",
      "AUC score: 0.6603773584905661\n",
      "finding best model parameters for fold number 4\n",
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 3ms/step - loss: 0.6505 - auc_16: 0.6648\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5143 - auc_16: 0.8802\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4145 - auc_16: 0.9275\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3577 - auc_16: 0.9385\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2940 - auc_16: 0.9546\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2380 - auc_16: 0.9732\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2235 - auc_16: 0.9731\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1986 - auc_16: 0.9782\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1502 - auc_16: 0.9883\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1324 - auc_16: 0.9928\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1174 - auc_16: 0.9952\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0842 - auc_16: 0.9987\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0863 - auc_16: 0.9985\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0784 - auc_16: 0.9985\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0621 - auc_16: 0.9985\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0429 - auc_16: 1.0000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0478 - auc_16: 0.9995\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0330 - auc_16: 1.0000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0342 - auc_16: 0.9997\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0288 - auc_16: 1.0000\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0243 - auc_16: 0.9999\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0210 - auc_16: 1.0000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0162 - auc_16: 1.0000\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0152 - auc_16: 1.0000\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0218 - auc_16: 0.9996\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0154 - auc_16: 1.0000\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0152 - auc_16: 1.0000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0093 - auc_16: 1.0000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0138 - auc_16: 1.0000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0115 - auc_16: 1.0000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0085 - auc_16: 1.0000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0060 - auc_16: 1.0000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0090 - auc_16: 1.0000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0075 - auc_16: 1.0000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0136 - auc_16: 1.0000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0043 - auc_16: 1.0000\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0060 - auc_16: 1.0000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0052 - auc_16: 1.0000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0061 - auc_16: 1.0000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0065 - auc_16: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0031 - auc_16: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0039 - auc_16: 1.0000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0031 - auc_16: 1.0000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0041 - auc_16: 1.0000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0042 - auc_16: 1.0000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0029 - auc_16: 1.0000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0025 - auc_16: 1.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0024 - auc_16: 1.0000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0028 - auc_16: 1.0000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0023 - auc_16: 1.0000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0023 - auc_16: 1.0000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0020 - auc_16: 1.0000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0027 - auc_16: 1.0000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0013 - auc_16: 1.0000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0028 - auc_16: 1.0000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0012 - auc_16: 1.0000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0030 - auc_16: 1.0000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0018 - auc_16: 1.0000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0017 - auc_16: 1.0000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0064 - auc_16: 1.0000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0053 - auc_16: 1.0000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0015 - auc_16: 1.0000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0028 - auc_16: 1.0000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0010 - auc_16: 1.0000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0016 - auc_16: 1.0000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0020 - auc_16: 1.0000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - auc_16: 1.0000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0026 - auc_16: 1.0000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - auc_16: 1.0000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0010 - auc_16: 1.0000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0012 - auc_16: 1.0000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0017 - auc_16: 1.0000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.8256e-04 - auc_16: 1.0000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0017 - auc_16: 1.0000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - auc_16: 1.0000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - auc_16: 1.0000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0015 - auc_16: 1.0000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.6815e-04 - auc_16: 1.0000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.8593e-04 - auc_16: 1.0000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0015 - auc_16: 1.0000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0013 - auc_16: 1.0000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.1688e-04 - auc_16: 1.0000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0010 - auc_16: 1.0000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.7217e-04 - auc_16: 1.0000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0010 - auc_16: 1.0000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - auc_16: 1.0000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.8940e-04 - auc_16: 1.0000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - auc_16: 1.0000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.4472e-04 - auc_16: 1.0000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.7446e-04 - auc_16: 1.0000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.7403e-04 - auc_16: 1.0000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - auc_16: 1.0000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.8482e-04 - auc_16: 1.0000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.7665e-04 - auc_16: 1.0000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.2125e-04 - auc_16: 1.0000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.0387e-04 - auc_16: 1.0000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.5856e-04 - auc_16: 1.0000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.5648e-04 - auc_16: 1.0000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.5797e-04 - auc_16: 1.0000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.8868e-04 - auc_16: 1.0000\n",
      "evaluating model for fold number 4\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 4ms/step - loss: 0.5908 - auc_17: 0.4551\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3864 - auc_17: 0.5516\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4064 - auc_17: 0.6299\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3933 - auc_17: 0.7114\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3343 - auc_17: 0.8233\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2944 - auc_17: 0.8787\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2869 - auc_17: 0.8647\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2690 - auc_17: 0.8985\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2659 - auc_17: 0.9043\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2528 - auc_17: 0.9085\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2156 - auc_17: 0.9537\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2043 - auc_17: 0.9408\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2023 - auc_17: 0.9423\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1904 - auc_17: 0.9511\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1683 - auc_17: 0.9709\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1584 - auc_17: 0.9744\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1600 - auc_17: 0.9728\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1329 - auc_17: 0.9810\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1472 - auc_17: 0.9757\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1238 - auc_17: 0.9869\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1212 - auc_17: 0.9813\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0963 - auc_17: 0.9940\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1030 - auc_17: 0.9944\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0836 - auc_17: 0.9965\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0868 - auc_17: 0.9962\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0824 - auc_17: 0.9941\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0644 - auc_17: 0.9972\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0799 - auc_17: 0.9934\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0566 - auc_17: 0.9988\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0545 - auc_17: 0.9995\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0566 - auc_17: 0.9988\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0441 - auc_17: 0.9996\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0263 - auc_17: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0302 - auc_17: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0274 - auc_17: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0261 - auc_17: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0249 - auc_17: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0235 - auc_17: 0.9998\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0223 - auc_17: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0205 - auc_17: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0184 - auc_17: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0128 - auc_17: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0199 - auc_17: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0151 - auc_17: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0107 - auc_17: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0100 - auc_17: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0096 - auc_17: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0091 - auc_17: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0099 - auc_17: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0074 - auc_17: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0063 - auc_17: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0082 - auc_17: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0066 - auc_17: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0076 - auc_17: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0050 - auc_17: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0128 - auc_17: 0.9999\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0084 - auc_17: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0079 - auc_17: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0058 - auc_17: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0088 - auc_17: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0044 - auc_17: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0054 - auc_17: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0055 - auc_17: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0036 - auc_17: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0068 - auc_17: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0040 - auc_17: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0040 - auc_17: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0093 - auc_17: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0035 - auc_17: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0050 - auc_17: 1.0000\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0031 - auc_17: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0028 - auc_17: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0032 - auc_17: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0027 - auc_17: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0021 - auc_17: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0028 - auc_17: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0025 - auc_17: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0047 - auc_17: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0019 - auc_17: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0014 - auc_17: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0017 - auc_17: 1.0000\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0024 - auc_17: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0024 - auc_17: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0013 - auc_17: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0011 - auc_17: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0026 - auc_17: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0015 - auc_17: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0019 - auc_17: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0011 - auc_17: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0016 - auc_17: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0021 - auc_17: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0020 - auc_17: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0012 - auc_17: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 9.8860e-04 - auc_17: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0017 - auc_17: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0013 - auc_17: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0014 - auc_17: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0023 - auc_17: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 6.0217e-04 - auc_17: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0015 - auc_17: 1.0000\n",
      "parameters: {'kerasclassifier__batch_size': 100, 'kerasclassifier__dropout_rate': 0.30000000000000004, 'kerasclassifier__epochs': 100, 'kerasclassifier__layer_size_factor': 8, 'kerasclassifier__num_hidden_layers': 2}\n",
      "AUC score: 0.7028301886792453\n",
      "finding best model parameters for fold number 5\n",
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 3ms/step - loss: 0.6860 - auc_18: 0.5848\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7006 - auc_18: 0.5714\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6860 - auc_18: 0.5680\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7040 - auc_18: 0.6022\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6860 - auc_18: 0.5830\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6825 - auc_18: 0.5961\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6796 - auc_18: 0.6308\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6836 - auc_18: 0.6262\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6669 - auc_18: 0.6361\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6353 - auc_18: 0.7059\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6448 - auc_18: 0.6823\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6517 - auc_18: 0.6786\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6388 - auc_18: 0.7068\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6391 - auc_18: 0.7131\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6327 - auc_18: 0.7329\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6293 - auc_18: 0.7264\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6125 - auc_18: 0.7417\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6213 - auc_18: 0.7306\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6079 - auc_18: 0.7558\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6177 - auc_18: 0.7228\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5969 - auc_18: 0.7885\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5801 - auc_18: 0.8014\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6021 - auc_18: 0.7557\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5986 - auc_18: 0.7746\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5704 - auc_18: 0.7998\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5856 - auc_18: 0.7870\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5749 - auc_18: 0.8016\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5406 - auc_18: 0.8467\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5298 - auc_18: 0.8453\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5219 - auc_18: 0.8464\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5352 - auc_18: 0.8220\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5490 - auc_18: 0.8161\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5390 - auc_18: 0.8245\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4947 - auc_18: 0.8560\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5279 - auc_18: 0.8249\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4825 - auc_18: 0.8787\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4995 - auc_18: 0.8540\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4717 - auc_18: 0.8733\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4711 - auc_18: 0.8723\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4352 - auc_18: 0.8947\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4358 - auc_18: 0.8929\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4859 - auc_18: 0.8683\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4721 - auc_18: 0.8681\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4481 - auc_18: 0.8902\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4225 - auc_18: 0.8965\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4166 - auc_18: 0.9062\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4219 - auc_18: 0.9032\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4037 - auc_18: 0.9071\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4489 - auc_18: 0.8742\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3732 - auc_18: 0.9247\n",
      "evaluating model for fold number 5\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 2ms/step - loss: 0.8724 - auc_19: 0.5557\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7928 - auc_19: 0.6464\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8114 - auc_19: 0.5540\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7838 - auc_19: 0.4301\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7544 - auc_19: 0.5235\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7319 - auc_19: 0.4522\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6816 - auc_19: 0.5182\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6831 - auc_19: 0.4337\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6827 - auc_19: 0.4917\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6239 - auc_19: 0.5281\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6531 - auc_19: 0.4806\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6400 - auc_19: 0.5009\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6038 - auc_19: 0.5366\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6221 - auc_19: 0.3955\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6553 - auc_19: 0.3815\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6297 - auc_19: 0.4499\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5707 - auc_19: 0.5441\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6138 - auc_19: 0.4968\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5647 - auc_19: 0.4999\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5960 - auc_19: 0.4350\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5596 - auc_19: 0.5670\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5325 - auc_19: 0.6022\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5492 - auc_19: 0.5194\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5435 - auc_19: 0.5198\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4955 - auc_19: 0.6247\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5523 - auc_19: 0.4777\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5950 - auc_19: 0.3633\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4850 - auc_19: 0.6199\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4803 - auc_19: 0.6270\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5341 - auc_19: 0.5285\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5236 - auc_19: 0.5356\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4925 - auc_19: 0.5344\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5145 - auc_19: 0.5220\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4951 - auc_19: 0.5663\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5041 - auc_19: 0.5244\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4916 - auc_19: 0.5591\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4502 - auc_19: 0.6265\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4628 - auc_19: 0.5935\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4918 - auc_19: 0.5294\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4244 - auc_19: 0.6198\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4692 - auc_19: 0.5554\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4312 - auc_19: 0.6576\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4088 - auc_19: 0.6662\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4895 - auc_19: 0.5045\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4820 - auc_19: 0.5308\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4371 - auc_19: 0.6088\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4511 - auc_19: 0.5926\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4310 - auc_19: 0.6504\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4236 - auc_19: 0.6279\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4072 - auc_19: 0.6267\n",
      "parameters: {'kerasclassifier__batch_size': 100, 'kerasclassifier__dropout_rate': 0.4, 'kerasclassifier__epochs': 50, 'kerasclassifier__layer_size_factor': 4, 'kerasclassifier__num_hidden_layers': 3}\n",
      "AUC score: 0.7439353099730458\n"
     ]
    }
   ],
   "source": [
    "fname = 'experiments/keras_models' + '_' + str(datetime.now().year) + '_' + str(datetime.now().month) \\\n",
    "    + '_' + str(datetime.now().day) + '_' + str(datetime.now().hour) + '_' + \\\n",
    "    str(datetime.now().minute) + '.txt'\n",
    "    \n",
    "eval_keras_params(fname = fname, param_grid = keras_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4139d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyperparameters based on tuning\n",
    "\n",
    "keras_params = {'layer_size_factor': 4, \n",
    "             'num_hidden_layers': 3,\n",
    "             'dropout_rate': 0.4}\n",
    "svm_params = {'C': 1, \n",
    "             'gamma': 0.0001, \n",
    "             'kernel': 'rbf', \n",
    "             'probability': True}\n",
    "rf_params = {'max_depth': None,\n",
    "            'max_features': 'auto', \n",
    "            'min_samples_leaf': 2, \n",
    "            'min_samples_split': 5, \n",
    "            'n_estimators': 140}\n",
    "logreg_params = {'C': 0.1,\n",
    "                'max_iter': 1000, \n",
    "                'penalty': 'l2'}\n",
    "\n",
    "final_models = [\n",
    "    ['svm', SVC(**svm_params)],\n",
    "    ['rf', RandomForestClassifier(**rf_params)], \n",
    "    ['logreg', LogisticRegression(**logreg_params)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0676c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to fit model and save it\n",
    "\n",
    "def fit_save_model(name, model, X, y):\n",
    "    print('fitting {}'.format(name))\n",
    "    model.fit(X, y)\n",
    "    fname = 'models/final_{}.sav'.format(name)\n",
    "    pickle.dump(model, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99111537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit final models on all training data and save\n",
    "\n",
    "sm = SMOTE()\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "for name, model in final_models:\n",
    "    fit_save_model(name = name, model = model, X = X_res, y = y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f69646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 83)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                1344      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,905\n",
      "Trainable params: 1,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 2ms/step - loss: 0.8705 - auc_3: 0.5243\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7870 - auc_3: 0.5113\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.7340 - auc_3: 0.5819\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7126 - auc_3: 0.5596\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7206 - auc_3: 0.5621\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.7091 - auc_3: 0.5454\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6975 - auc_3: 0.5474\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6887 - auc_3: 0.5740\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6822 - auc_3: 0.5623\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6995 - auc_3: 0.5623\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6836 - auc_3: 0.5513\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6687 - auc_3: 0.5875\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6553 - auc_3: 0.6285\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6658 - auc_3: 0.6273\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6674 - auc_3: 0.5877\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6541 - auc_3: 0.6354\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6473 - auc_3: 0.6679\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6597 - auc_3: 0.6289\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6532 - auc_3: 0.6532\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6326 - auc_3: 0.7039\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6382 - auc_3: 0.6740\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6270 - auc_3: 0.6949\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6124 - auc_3: 0.7094\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6043 - auc_3: 0.7195\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5938 - auc_3: 0.7450\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5915 - auc_3: 0.7308\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5639 - auc_3: 0.7859\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6059 - auc_3: 0.7258\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5590 - auc_3: 0.8019\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.5806 - auc_3: 0.7646\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5858 - auc_3: 0.7603\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5548 - auc_3: 0.7924\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5071 - auc_3: 0.8371\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5154 - auc_3: 0.8248\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4798 - auc_3: 0.8634\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4713 - auc_3: 0.8674\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4839 - auc_3: 0.8431\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4946 - auc_3: 0.8580\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4618 - auc_3: 0.8698\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4583 - auc_3: 0.8709\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4681 - auc_3: 0.8621\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4657 - auc_3: 0.8719\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4265 - auc_3: 0.8949\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4439 - auc_3: 0.8719\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4054 - auc_3: 0.9129\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4077 - auc_3: 0.9071\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4074 - auc_3: 0.9045\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4009 - auc_3: 0.9045\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3731 - auc_3: 0.9185\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3916 - auc_3: 0.9088\n",
      "INFO:tensorflow:Assets written to: models/final_mlp/assets\n"
     ]
    }
   ],
   "source": [
    "# fit keras model on all training data and save\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "\n",
    "final_mlp = build_model(input_shape = X_train.shape[1], **keras_params)\n",
    "final_mlp.summary()\n",
    "final_mlp.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [AUC()])\n",
    "final_mlp.fit(X_res, y_res, batch_size = batch_size, epochs = epochs)\n",
    "final_mlp.save('models/final_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42a0c8",
   "metadata": {},
   "source": [
    "## Evaluate ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642fb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ensemble models\n",
    "\n",
    "mlp_ensemble = KerasClassifier(build_fn = build_model, input_shape = X_train.shape[1], **keras_params)\n",
    "mlp_ensemble._estimator_type = 'classifier'\n",
    "svm_ensemble = SVC(**svm_params)\n",
    "rf_ensemble = RandomForestClassifier(**rf_params)\n",
    "logreg_ensemble = LogisticRegression(**logreg_params)\n",
    "\n",
    "ensemble1_params = [{'estimators': [('mlp', mlp_ensemble), \n",
    "                                    ('svm', svm_ensemble)], \n",
    "                     'voting': 'soft'}]\n",
    "ensemble2_params = [{'estimators': [('mlp', mlp_ensemble), \n",
    "                                    ('rf', rf_ensemble)], \n",
    "                     'voting': 'soft'}]\n",
    "ensemble3_params = [{'estimators': [('mlp', mlp_ensemble), \n",
    "                                    ('logreg', logreg_ensemble)],\n",
    "                     'voting': 'soft'}]\n",
    "ensemble4_params = [{'estimators': [('svm', svm_ensemble), \n",
    "                                    ('rf', rf_ensemble)], \n",
    "                     'voting': 'soft'}]\n",
    "ensemble5_params = [{'estimators': [('svm', svm_ensemble),  \n",
    "                                    ('logreg', logreg_ensemble)], \n",
    "                     'voting': 'soft'}]\n",
    "ensemble6_params = [{'estimators': [('rf', rf_ensemble),\n",
    "                                    ('logreg', logreg_ensemble)], \n",
    "                     'voting': 'soft'}]\n",
    "\n",
    "ensemble_models = [\n",
    "    ['Ensemble 1 (mlp/svm)', VotingClassifier, ensemble1_params],\n",
    "    ['Ensemble 2 (mlp/rf)', VotingClassifier, ensemble2_params],\n",
    "    ['Ensemble 3 (mlp/logreg)', VotingClassifier, ensemble3_params],\n",
    "    ['Ensemble 4 (svm/rf)', VotingClassifier, ensemble4_params],\n",
    "    ['Ensemble 5 (svm/logreg)', VotingClassifier, ensemble5_params],\n",
    "    ['Ensemble 6 (rf/logreg)', VotingClassifier, ensemble6_params],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f580905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/imblearn/pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_voting.py\", line 324, in fit\n",
      "    return super().fit(X, transformed_y, sample_weight)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_voting.py\", line 65, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_base.py\", line 262, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator KerasClassifier should be a classifier.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/imblearn/pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_voting.py\", line 324, in fit\n",
      "    return super().fit(X, transformed_y, sample_weight)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_voting.py\", line 65, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_base.py\", line 262, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator KerasClassifier should be a classifier.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/imblearn/pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_voting.py\", line 324, in fit\n",
      "    return super().fit(X, transformed_y, sample_weight)\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_voting.py\", line 65, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"/Users/haydnhoffman/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_base.py\", line 262, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator KerasClassifier should be a classifier.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-36b1375b89da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0meval_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-d80ce1ae6ea8>\u001b[0m in \u001b[0;36meval_models\u001b[0;34m(models, score, X_train, y_train, fname)\u001b[0m\n\u001b[1;32m     12\u001b[0m             )\n\u001b[1;32m     13\u001b[0m             \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepeatedStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    268\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    269\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m     75\u001b[0m             delayed(_fit_single_estimator)(\n\u001b[1;32m     76\u001b[0m                 \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_INT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             trees = [\n\u001b[0m\u001b[1;32m    440\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             trees = [\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             ]\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0msub\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_object_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mdeep_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"__\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeep_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluate ensemble models\n",
    "\n",
    "scores = ['roc_auc', 'precision', 'recall', 'f1', 'accuracy']\n",
    "fname = 'experiments/sklearn_ensembles' + '_' + str(datetime.now().year) + '_' + str(datetime.now().month) \\\n",
    "    + '_' + str(datetime.now().day) + '_' + str(datetime.now().hour) + '_' + \\\n",
    "    str(datetime.now().minute) + '.txt'\n",
    "    \n",
    "for score in scores:\n",
    "    eval_models(models = ensemble_models, score = score, X_train = X_train, y_train = y_train, fname = fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f0dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
